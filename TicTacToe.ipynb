{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUxOK1GBUX88KG0Y0OWO1X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahiinn62/DLRL_assignment/blob/main/TicTacToe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tic_tac_toe_rl.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self, p1, p2):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS), dtype=int)\n",
        "        self.p1 = p1\n",
        "        self.p2 = p2\n",
        "        self.isEnd = False\n",
        "        self.boardHash = None\n",
        "\n",
        "        self.playerSymbol = 1\n",
        "\n",
        "\n",
        "    def getHash(self) -> str:\n",
        "        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
        "        return self.boardHash\n",
        "\n",
        "    def winner(self):\n",
        "        # row\n",
        "        for i in range(BOARD_ROWS):\n",
        "            row_sum = sum(self.board[i, :])\n",
        "            if row_sum == 3:\n",
        "                self.isEnd = True\n",
        "                return 1\n",
        "            if row_sum == -3:\n",
        "                self.isEnd = True\n",
        "                return -1\n",
        "        # col\n",
        "        for i in range(BOARD_COLS):\n",
        "            col_sum = sum(self.board[:, i])\n",
        "            if col_sum == 3:\n",
        "                self.isEnd = True\n",
        "                return 1\n",
        "            if col_sum == -3:\n",
        "                self.isEnd = True\n",
        "                return -1\n",
        "        # diagonal\n",
        "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
        "        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
        "        if diag_sum1 == 3 or diag_sum2 == 3:\n",
        "            self.isEnd = True\n",
        "            return 1\n",
        "        if diag_sum1 == -3 or diag_sum2 == -3:\n",
        "            self.isEnd = True\n",
        "            return -1\n",
        "\n",
        "\n",
        "        if len(self.availablePositions()) == 0:\n",
        "            self.isEnd = True\n",
        "            return 0\n",
        "\n",
        "\n",
        "        self.isEnd = False\n",
        "        return None\n",
        "\n",
        "    def availablePositions(self):\n",
        "        positions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.board[i, j] == 0:\n",
        "                    positions.append((i, j))\n",
        "        return positions\n",
        "\n",
        "    def updateState(self, position):\n",
        "        # place current player's symbol then flip symbol\n",
        "        self.board[position] = self.playerSymbol\n",
        "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
        "\n",
        "    # only when game ends\n",
        "    def giveReward(self):\n",
        "        result = self.winner()\n",
        "        # backpropagate reward (p1 is positive player)\n",
        "        if result == 1:\n",
        "            self.p1.feedReward(1)\n",
        "            self.p2.feedReward(0)\n",
        "        elif result == -1:\n",
        "            self.p1.feedReward(0)\n",
        "            self.p2.feedReward(1)\n",
        "        else:  # draw\n",
        "            self.p1.feedReward(0.5)\n",
        "            self.p2.feedReward(0.5)\n",
        "\n",
        "    # board reset\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS), dtype=int)\n",
        "        self.boardHash = None\n",
        "        self.isEnd = False\n",
        "        self.playerSymbol = 1\n",
        "\n",
        "    def play(self, rounds=5000, verbose_every: int = 1000):\n",
        "        \"\"\"\n",
        "        Self-play training loop. p1 and p2 learn from rewards.\n",
        "        \"\"\"\n",
        "        for i in range(rounds):\n",
        "            if verbose_every and i % verbose_every == 0:\n",
        "                print(f\"Training round: {i}/{rounds}\")\n",
        "            while not self.isEnd:\n",
        "                # Player 1 turn\n",
        "                positions = self.availablePositions()\n",
        "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "                self.updateState(p1_action)\n",
        "                board_hash = self.getHash()\n",
        "                self.p1.addState(board_hash)\n",
        "\n",
        "                win = self.winner()\n",
        "                if win is not None:\n",
        "                    self.giveReward()\n",
        "                    self.p1.reset()\n",
        "                    self.p2.reset()\n",
        "                    self.reset()\n",
        "                    break\n",
        "\n",
        "                # Player 2 turn\n",
        "                positions = self.availablePositions()\n",
        "                p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
        "                self.updateState(p2_action)\n",
        "                board_hash = self.getHash()\n",
        "                self.p2.addState(board_hash)\n",
        "\n",
        "                win = self.winner()\n",
        "                if win is not None:\n",
        "                    self.giveReward()\n",
        "                    self.p1.reset()\n",
        "                    self.p2.reset()\n",
        "                    self.reset()\n",
        "                    break\n",
        "\n",
        "    # play with human\n",
        "    def play2(self):\n",
        "        \"\"\"\n",
        "        Human vs computer interactive play. Assumes p1 is computer, p2 is human in code below.\n",
        "        \"\"\"\n",
        "        while not self.isEnd:\n",
        "            # Computer or player1\n",
        "            positions = self.availablePositions()\n",
        "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "            self.updateState(p1_action)\n",
        "            self.showBoard()\n",
        "            win = self.winner()\n",
        "            if win is not None:\n",
        "                if win == 1:\n",
        "                    print(f\"{self.p1.name} wins!\")\n",
        "                else:\n",
        "                    print(\"Tie!\")\n",
        "                self.reset()\n",
        "                break\n",
        "\n",
        "            # Human (p2)\n",
        "            positions = self.availablePositions()\n",
        "            p2_action = self.p2.chooseAction(positions)\n",
        "            self.updateState(p2_action)\n",
        "            self.showBoard()\n",
        "            win = self.winner()\n",
        "            if win is not None:\n",
        "                if win == -1:\n",
        "                    print(f\"{self.p2.name} wins!\")\n",
        "                else:\n",
        "                    print(\"Tie!\")\n",
        "                self.reset()\n",
        "                break\n",
        "\n",
        "    def showBoard(self):\n",
        "        for i in range(BOARD_ROWS):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'x'\n",
        "                elif self.board[i, j] == -1:\n",
        "                    token = 'o'\n",
        "                else:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')\n",
        "\n",
        "\n",
        "class Player:\n",
        "    def __init__(self, name, exp_rate=0.3, lr=0.2, decay_gamma=0.9):\n",
        "        self.name = name\n",
        "        self.states = []  # record all positions taken (hashes)\n",
        "        self.lr = lr\n",
        "        self.exp_rate = exp_rate\n",
        "        self.decay_gamma = decay_gamma\n",
        "        self.states_value = {}  # state -> value\n",
        "\n",
        "    def getHash(self, board):\n",
        "        return str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
        "\n",
        "    def chooseAction(self, positions, current_board=None, symbol=1):\n",
        "        \"\"\"\n",
        "        Choose action based on epsilon-greedy and learned state-values.\n",
        "        positions: list of (i,j) tuples\n",
        "        \"\"\"\n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            # exploration\n",
        "            idx = np.random.choice(len(positions))\n",
        "            action = positions[idx]\n",
        "        else:\n",
        "\n",
        "            value_max = -np.inf\n",
        "            action = positions[0]\n",
        "            for p in positions:\n",
        "                next_board = current_board.copy()\n",
        "                next_board[p] = symbol\n",
        "                next_boardHash = self.getHash(next_board)\n",
        "                value = self.states_value.get(next_boardHash, 0)\n",
        "                if value >= value_max:\n",
        "                    value_max = value\n",
        "                    action = p\n",
        "        return action\n",
        "\n",
        "    def addState(self, state_hash):\n",
        "        self.states.append(state_hash)\n",
        "\n",
        "    def feedReward(self, reward):\n",
        "        \"\"\"\n",
        "        Backpropagate reward to visited states (reverse order).\n",
        "        \"\"\"\n",
        "        for st in reversed(self.states):\n",
        "            if self.states_value.get(st) is None:\n",
        "                self.states_value[st] = 0\n",
        "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
        "            reward = self.states_value[st]\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "\n",
        "    def savePolicy(self, folder: str = \".\"):\n",
        "        folder_path = Path(folder)\n",
        "        folder_path.mkdir(parents=True, exist_ok=True)\n",
        "        file_path = folder_path / f\"policy_{self.name}.pkl\"\n",
        "        with open(file_path, 'wb') as fw:\n",
        "            pickle.dump(self.states_value, fw)\n",
        "        print(f\"Policy saved to {file_path}\")\n",
        "\n",
        "    def loadPolicy(self, file: str):\n",
        "        file_path = Path(file)\n",
        "        if not file_path.exists():\n",
        "            print(f\"Policy file {file} not found. Continuing without loading.\")\n",
        "            return\n",
        "        with open(file_path, 'rb') as fr:\n",
        "            self.states_value = pickle.load(fr)\n",
        "        print(f\"Policy loaded from {file_path}\")\n",
        "\n",
        "\n",
        "class HumanPlayer:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def chooseAction(self, positions):\n",
        "        \"\"\"\n",
        "        Ask the human for a move until a valid one is provided.\n",
        "        Accepts zero-based row/col integers.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                row = int(input(\"Input your action row (0/1/2): \"))\n",
        "                col = int(input(\"Input your action col (0/1/2): \"))\n",
        "                action = (row, col)\n",
        "                if action in positions:\n",
        "                    return action\n",
        "                print(\"Invalid position or already taken. Try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please input integer indices 0,1 or 2.\")\n",
        "\n",
        "    def addState(self, state):\n",
        "        pass\n",
        "\n",
        "    def feedReward(self, reward):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # training\n",
        "    print(\"Creating players and state...\")\n",
        "    p1 = Player(name=\"p1\")\n",
        "    p2 = Player(name=\"p2\")\n",
        "    st = State(p1, p2)\n",
        "    print(f\"Training for {args.rounds} rounds... (this may take a moment)\")\n",
        "    st.play(rounds=args.rounds, verbose_every=args.verbose_every)\n",
        "\n",
        "    # Save learned policy for p1 (first player)\n",
        "    p1.savePolicy(folder=args.policy_dir)\n",
        "\n",
        "    # Play against human\n",
        "    print(\"\\nNow you can play against the trained agent (p1).\")\n",
        "    agent = Player(\"computer\", exp_rate=0)  # deterministic agent\n",
        "    agent.loadPolicy(str(Path(args.policy_dir) / \"policy_p1.pkl\"))\n",
        "    human = HumanPlayer(\"human\")\n",
        "    game_state = State(agent, human)\n",
        "\n",
        "    cont = 'y'\n",
        "    while cont.lower() == 'y':\n",
        "        game_state.play2()\n",
        "        cont = input(\"Play again? (y/n): \").strip() or 'n'\n",
        "    print(\"Thanks for playing!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    try:\n",
        "        parser = argparse.ArgumentParser(description=\"Tic-Tac-Toe RL demo\", add_help=True)\n",
        "        parser.add_argument(\"--rounds\", type=int, default=5000, help=\"Number of self-play training rounds\")\n",
        "        parser.add_argument(\"--verbose_every\", type=int, default=1000, help=\"Print training progress every N rounds\")\n",
        "        parser.add_argument(\"--policy_dir\", type=str, default=\"policies\", help=\"Directory to save/load policies\")\n",
        "        args, unknown = parser.parse_known_args()\n",
        "    except Exception:\n",
        "\n",
        "        class Args: pass\n",
        "        args = Args()\n",
        "        args.rounds = 5000\n",
        "        args.verbose_every = 1000\n",
        "        args.policy_dir = \"policies\"\n",
        "\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWSaMtijUIjm",
        "outputId": "2d8b8692-f061-4f42-8a55-ad387c64d0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating players and state...\n",
            "Training for 5000 rounds... (this may take a moment)\n",
            "Training round: 0/5000\n",
            "Training round: 1000/5000\n",
            "Training round: 2000/5000\n",
            "Training round: 3000/5000\n",
            "Training round: 4000/5000\n",
            "Policy saved to policies/policy_p1.pkl\n",
            "\n",
            "Now you can play against the trained agent (p1).\n",
            "Policy loaded from policies/policy_p1.pkl\n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | x |   | \n",
            "-------------\n",
            "Input your action row (0/1/2): 0\n",
            "Input your action col (0/1/2): 1\n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | x |   | \n",
            "-------------\n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x | x |   | \n",
            "-------------\n",
            "Input your action row (0/1/2): 1\n",
            "Input your action col (0/1/2): 2\n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "|   |   | o | \n",
            "-------------\n",
            "| x | x |   | \n",
            "-------------\n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "|   |   | o | \n",
            "-------------\n",
            "| x | x | x | \n",
            "-------------\n",
            "computer wins!\n",
            "Play again? (y/n): n\n",
            "Thanks for playing!\n"
          ]
        }
      ]
    }
  ]
}